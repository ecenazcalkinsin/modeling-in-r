---
title: "Homework04"
format: html
editor: visual
---

```{r}
#| include: false
library(magrittr)
library(tidyverse)
library(ISLR2)
library(ggplot2)

```

## Ece Naz Çalkınsın 22003342 HW4

a\) Fit a multiple regression model to predict Sales using Price, Urban, and US.

```{r}
reg_model <- lm(formula = Sales ~ Price + Urban + US, data = Carseats)
```

b\) Provide an interpretation of each coefficient in the model.

```{r}
summary(reg_model)
```

As p-value for price is smaller than 0.05, it affects sales. They have negative relation as expected.

Urban is not significant for sales because of the p-value. It is nearly 1. Hence, there is no strong relation between urban stores and sales.

For US, it is also significant since p-value is less than 0.05. US and sales have a positive relation with each other. If there is a store in US, the sales is also increased.

c\) Write out the model in equation form, being careful to handle the qualitative variables properly.

$$ Sales = 13.043469 - 0.054459 * Price -0.021916 * Urban + 1.200573 * US + \epsilon $$

d\) For which of the predictors can you reject the null hypothesis H\_{0}:β\_{j} = 0 ?

When the p-value is smaller than 0.05, this means that we can reject the null hypothesis. This suggests that a real effect or relationship exists between the predictor and the response variables. Hence, in this case, for US and price we can reject the null hypothesis.

e\) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.

```{r}
smaller_model <- lm(formula = Sales ~ US + Price, data = Carseats)
```

f\) How well do the models in (a) and (e) fit the data?

```{r}
summary(smaller_model)
```

For testing how well the model fits to the data, we should firstly look at the Adjusted R-squared results. The one with higher Adjusted R-squared result indicates a better fit. So, the smaller model has a higher value and it is a better fit. However, since the difference is so small, we can also look at other indicators like RSE. When RSE is low, it means the model is a better fit. Smaller model has a smaller value of RSE, so it is a better fit compared to original model.

g\) Using the model from (e), obtain 95% confidence intervals for the coefficient(s).

```{r}
confint(smaller_model, level = 0.95)
```

h\) Is there evidence of outliers or high leverage observations in the model from (e)?

```{r}
residuals <- residuals(smaller_model)
boxplot(residuals)
```

By creating a boxplot for smaller model, it is clearly seen that there are a few outliers in the graph. This boxplot is the evidence.

This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

a)  For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

**Crim-zn**

```{r}
zn_model <- lm(formula = crim ~ zn, data = Boston)
summary(zn_model)
plot(zn_model)
```

There a statistically significant association between zn and the response variable crim as p-value is smaller than 0.05. They have a negative correlation. In the Residuals vs Fitted graph, a downward trend as the fitted values increase, suggests a negative relationship which backs up my assertions.

**Crim-indus**

```{r}
indus_model <- lm(formula = crim ~ indus, data = Boston)
summary(indus_model)
plot(indus_model)
```

According to the low p-value and the coefficient, there is a positive association between indus and crime rate.The Scale-Location graph has a normal increase and Q-Q shows an increase from the diagonal line, which backs up the positive correlation.

**Crim-chas**

```{r}
chas_model <- lm(formula = crim ~ chas, data = Boston)
summary(chas_model)
plot(chas_model)
```

The p-value is higher than 0.05 so, there is no enough evidence that they are correlated.t: The spread of the residuals remains constant across levels of the fitted values, indicating that there is no specific relationship between the variables, as expected from the assertions.

**Crim-nox**

```{r}
nox_model <- lm(formula = crim ~ nox, data = Boston)
summary(nox_model)
plot(nox_model)
```

The p-value being lower than 0.05 indicates there is a correlation between nox and crime rate. The coefficient being positive means that this relation is a positive association. In the Q-Q graph, there is an increase from the diagonal line, which backs up the positive relation.

**Crim-rm**

```{r}
rm_model <- lm(formula = crim ~ rm, data = Boston)
summary(rm_model)
plot(rm_model)
```

There is a negative relation between crim and rm because of the low p-value and negative coefficient.

**Crim-age**

```{r}
age_model <- lm(formula = crim ~ age, data = Boston)
summary(age_model)
plot(age_model)
```

The p-value is lower than 0.05 so there is an association between age and crime rate. This association is positive. Again, Q-Q graph backs this up because of the increase from the diagonal line.

**Crim-dis**

```{r}
dis_model <- lm(formula = crim ~ dis, data = Boston)
summary(dis_model)
plot(dis_model)
```

There is a negative association between the variables since p-value is lower than 0.05 and coefficient is negative. In Residuals vs Fitted graph, there seems to be a decrease when fitted values increase. This also indicates a negative relation. Same for the Scale-Location graph.

**Crim-rad**

```{r}
rad_model <- lm(formula = crim ~ rad, data = Boston)
summary(rad_model)
plot(rad_model)
```

There is a positive association between the variables since p-value is lower than 0.05 and coefficient is positive In Scale-Location graph, there seems to be an increase when fitted values increase. This also indicates a negative relation. For the Q-Q graph, again there is an increase from the diagonal line.

**Crim-tax**

```{r}
tax_model <- lm(formula = crim ~ tax, data = Boston)
summary(tax_model)
plot(tax_model)
```

The p-value is lower than 0.05 so there is an association between tax and crime rate. This association is positive. Again, Q-Q graph backs this up because of the increase from the diagonal line.

**Crim-ptratio**

```{r}
ptratio_model <- lm(formula = crim ~ ptratio, data = Boston)
summary(ptratio_model)
plot(ptratio_model)
```

According to the low p-value and the positive coefficient, there is a positive association between ptratio and crime rate.The Scale-Location graph has a normal increase and Q-Q shows an increase from the diagonal line, which backs up the positive correlation. There is also an increase in the Scale-Location graph which also justifies the positive relation.

**Crim-lstat**

```{r}
lstat_model <- lm(formula = crim ~ lstat, data = Boston)
summary(lstat_model)
plot(lstat_model)
```

According to the low p-value and the positive coefficient, there is a positive association between lstat and crim. The Scale-Location graph has a normal increase and justifies the positive relation.

**Crim-medv**

```{r}
medv_model <- lm(formula = crim ~ medv, data = Boston)
summary(medv_model)
plot(medv_model)
```

There a statistically significant association between medv and the response variable crim as p-value is smaller than 0.05. They have a negative correlation as coefficient is smaller than zero. In the Residuals vs Fitted graph, a downward trend as the fitted values increase, suggests a negative relationship which backs up my assertions.

b\) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $$H_{0}:β_{j} = 0 $$?

```{r}
mult_model <- lm(crim ~.,data = Boston)
summary(mult_model)
```

When the p-value is smaller than the chosen significance level (0.05), it indicates that the observed data provides enough evidence to reject the null hypothesis. Hence, zn, dis,rad and medv we can reject the null hypothesis.

c\) How do your results from (a) compare to your results from (b) ? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point on the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

```{r}

univariate_models <- lapply(setdiff(names(Boston), "crim"), function(x) lm(reformulate(x, "crim"), data = Boston))

univariate_coef <- map_dbl(univariate_models, ~ coef(.)[-1])
multiple_coef <- coef(mult_model)[-1]

comparison <- data.frame(Simple_Coefficients = unlist(univariate_coef),
                            Multiple_Coefficients = multiple_coef)

ggplot(comparison, aes(x = Simple_Coefficients, y = Multiple_Coefficients)) +
  geom_point(color="blue") 
  
```

d\) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form.

$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3+\epsilon $$

```{r}
cubic_model <- lapply(setdiff(names(Boston), "crim"), function(x) 
{
  if (length(unique(Boston[[x]])) < 4) 
  {
    return(NULL)
  }
  
  model <- lm(crim ~ poly(Boston[[x]], min(3, length(unique(Boston[[x]])))), data = Boston)
  print(summary(model))
})
```

For each predictor, first, second and third polynomial degree values are given with the above code. Looking specifically to each predictor's each polynomial p-values, linearity and non-linearity could be checked. For example, for the first one:

poly(Boston\[\[x\]\], min(3, length(unique(Boston\[\[x\]\]))))1 4.7e-06

poly(Boston\[\[x\]\], min(3, length(unique(Boston\[\[x\]\]))))2 0.00442

poly(Boston\[\[x\]\], min(3, length(unique(Boston\[\[x\]\]))))3 0.22954

These are the following p-values for respectively first, second and third degree polynomials. Since the smallest and the most significant one is the first one, it can be said that the first degree polynomial model fits the most. This means that there is linearity between crim and the first predictor, zn.
